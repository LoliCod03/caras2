<!doctype html>
<html lang="es">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Reconocimiento Facial IA ðŸ‘¤</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { background-color: #121212; color: white; font-family: sans-serif; }
        .contenedor-camara { 
            position: relative; width: 400px; height: 400px; 
            margin: 20px auto; border: 5px solid #00d1b2; border-radius: 15px; 
            overflow: hidden; background: #000;
        }
        #resultado { font-size: 2.5rem; color: #00d1b2; font-weight: bold; margin-top: 15px; }
        .status-badge { font-size: 0.9rem; padding: 10px; border-radius: 5px; margin-bottom: 20px; }
    </style>
</head>
<body>

<div class="container text-center mt-5">
    <h1>Reconocedor de Rostros</h1>
    <div id="status" class="status-badge alert alert-warning">Cargando Modelo Inteligente...</div>

    <div class="contenedor-camara">
        <canvas id="canvas" width="400" height="400"></canvas>
    </div>

    <div id="resultado">Esperando...</div>
    
    <button class="btn btn-outline-light mt-4" onclick="cambiarCamara()">ðŸ”„ Cambiar CÃ¡mara</button>

    <video id="video" playsinline autoplay style="width: 1px; visibility: hidden;"></video>
    <canvas id="othercanvas" width="150" height="150" style="display: none"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>

<script>
    var video = document.getElementById("video");
    var canvas = document.getElementById("canvas");
    var ctx = canvas.getContext("2d");
    var modelo = null;
    var currentStream = null;
    var facingMode = "user";

    // --- EL ORDEN DE TUS CARPETAS ---
    const PERSONAS = ["Daniela", "Fajardo", "Gabriel", "Idney", "JesÃºs", "Kevin", "Victor"];

    async function cargarModelo() {
        try {
            // 1. Cargamos el archivo JSON
            const resp = await fetch('./model.json');
            let modelJson = await resp.json();

            // 2. Parche de compatibilidad (Modifica el JSON en memoria)
            if (modelJson.modelTopology && modelJson.modelTopology.model_config) {
                let layers = modelJson.modelTopology.model_config.config.layers;
                if (layers && layers[0]) {
                    layers[0].config.batch_input_shape = [null, 150, 150, 3];
                }
            }

            // 3. Cargar el modelo usando los Shards (.bin)
            const modelUrl = window.location.href.substring(0, window.location.href.lastIndexOf('/') + 1);
            modelo = await tf.loadLayersModel(tf.io.fromMemory(
                modelJson.modelTopology,
                modelJson.weightsManifest,
                modelUrl
            ));

            document.getElementById("status").innerText = "Â¡IA Lista! Identificando...";
            document.getElementById("status").className = "status-badge alert alert-success";
            predecir(); // Iniciar escaneo
        } catch (error) {
            console.error(error);
            document.getElementById("status").innerText = "Error al cargar modelo (.bin o .json faltantes)";
            document.getElementById("status").className = "status-badge alert alert-danger";
        }
    }

    function mostrarCamara() {
        var constraints = { audio: false, video: { facingMode: facingMode, width: 400, height: 400 } };
        navigator.mediaDevices.getUserMedia(constraints)
            .then(function(stream) {
                currentStream = stream;
                video.srcObject = currentStream;
                procesarCamara();
            })
            .catch(function(err) { console.log("Error de cÃ¡mara: ", err); });
    }

    function cambiarCamara() {
        if (currentStream) { currentStream.getTracks().forEach(track => track.stop()); }
        facingMode = (facingMode == "user") ? "environment" : "user";
        mostrarCamara();
    }

    function procesarCamara() {
        ctx.drawImage(video, 0, 0, 400, 400, 0, 0, 400, 400);
        setTimeout(procesarCamara, 20);
    }

    async function predecir() {
        if (modelo != null) {
            const tempCanvas = document.getElementById("othercanvas");
            const tempCtx = tempCanvas.getContext("2d");

            // Redimensionar a 150x150 para la IA
            tempCtx.drawImage(canvas, 0, 0, 400, 400, 0, 0, 150, 150);

            const prediccion = tf.tidy(() => {
                const imgData = tempCtx.getImageData(0,0,150,150);
                const tensor = tf.browser.fromPixels(imgData)
                                 .toFloat()
                                 .div(tf.scalar(255.0))
                                 .expandDims(0);
                return modelo.predict(tensor).dataSync();
            });

            // Buscar quiÃ©n tiene el porcentaje mÃ¡s alto
            let maxIndex = prediccion.indexOf(Math.max(...prediccion));
            let confianza = prediccion[maxIndex];

            if (confianza > 0.7) { // 70% de seguridad mÃ­nimo
                document.getElementById("resultado").innerText = PERSONAS[maxIndex];
            } else {
                document.getElementById("resultado").innerText = "Desconocido";
            }
        }
        setTimeout(predecir, 500); // Escanear cada medio segundo
    }

    window.onload = () => {
        mostrarCamara();
        cargarModelo();
    };
</script>

</body>
</html>